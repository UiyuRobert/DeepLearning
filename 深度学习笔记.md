# 深度学习笔记

## 1 基础数据操作

访问元素时，有如下操作（一列：`[:,1]`）：

![image-20250915094539729](assets/image-20250915094539729.png)

## 2 PyTorch 数据操作

### `torch.utils.data.TensorDataset`

一个非常实用的数据集封装类，用于将多个张量组合成一个统一的数据集接口

```python
from torch.utils.data import TensorDataset
import torch

# 创建示例数据
features = torch.randn(100, 5)  # 100个样本，5个特征
labels = torch.randint(0, 2, (100,))  # 100个标签

# 创建 TensorDataset
dataset = TensorDataset(features, labels)
```

### `torch.randn()`

用于生成服从标准正态分布的随机数张量，定义如下：

```python
torch.randn(*size, *, out=None, dtype=None, layout=torch.strided, 
           device=None, requires_grad=False) → Tensor

# 生成标量（0维张量）
x = torch.randn(())
print(x)  # 例如: tensor(0.1234)

# 生成1维张量（向量）
x = torch.randn(5)
print(x)  # 例如: tensor([-0.2345, 1.5678, 0.8765, -1.2345, 0.5432])

# 生成2维张量（矩阵）
x = torch.randn(3, 4)
print(x)
```

### `torch.randint()`

用于生成在指定范围内均匀分布的随机整数的函数，定义如下：

```python
torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor
```

- **`low` (int, optional)**: 产生随机整数范围的**下限**（包含）。默认值是 `0`。
- **`high` (int)**: 产生随机整数范围的**上限**（**不包含**）。这是一个必须指定的参数。
- **`size` (tuple of ints)**: 定义了输出张量的**形状**。例如，`size=(2, 3)` 会生成一个 2行3列 的矩阵；`size=(5,)` 会生成一个长度为5的向量。

### `torch.utils.data.DataLoader`

负责高效地加载和预处理数据，并将其组织成批次供模型训练使用。

```python
DataLoader(
    dataset,            # 必须：Dataset对象
    batch_size=1,       # 批次大小
    shuffle=False,      # 是否打乱数据（训练集通常为True，测试集为False）
    num_workers=0,      # 加载数据的进程数（0=主进程，建议设为CPU核心数）
    pin_memory=False,   # 是否锁页内存，GPU训练时建议为True
    drop_last=False,    # 是否丢弃最后一个不完整的批次
)

import torch
from torch.utils.data import DataLoader, TensorDataset

# 1. 准备数据（这里用虚拟数据示例）
x = torch.randn(100, 3, 32, 32)  # 100个样本，3通道，32x32图像
y = torch.randint(0, 10, (100,))  # 100个标签

# 2. 创建 Dataset（这里使用TensorDataset）
dataset = TensorDataset(x, y)

# 3. 创建 DataLoader
dataloader = DataLoader(
    dataset=dataset,
    batch_size=32,      # 每个批次的样本数
    shuffle=True,       # 是否打乱数据
    num_workers=2       # 使用几个进程加载数据
)

# 4. 在训练循环中使用
for epoch in range(3):
    for batch_idx, (data, target) in enumerate(dataloader):
        # data.shape: torch.Size([32, 3, 32, 32])
        # target.shape: torch.Size([32])
        print(f'Epoch: {epoch}, Batch: {batch_idx}, Data shape: {data.shape}')
        
        # 这里通常是训练代码：
        # optimizer.zero_grad()
        # output = model(data)
        # loss = criterion(output, target)
        # loss.backward()
        # optimizer.step()
```

### `normal_()`

#### 1. 定义

```python
normal_(mean=0, std=1, *, generator=None) -> Tensor
```

- **`normal`**：指的是**正态分布**（也叫高斯分布），这是概率论和统计学中最重要的一种连续概率分布。
- **下划线 `_`**：在 PyTorch 中，函数名后面带下划线表示这是一个**原地操作（in-place operation）**。意思是这个函数会**直接修改调用它的张量本身**，而不是返回一个新的张量。

所以 `normal_()` 的意思是："用正态分布的值来填充这个张量本身"。

#### 2. 参数解释

- **`mean=0`**：正态分布的**均值**（平均值）。分布的中心位置，默认为0。
- **`std=1`**：正态分布的**标准差**。表示数据的离散程度，默认为1。
- **`generator=None`**：用于控制随机数生成的生成器。如果提供，可以确保随机性的可重复性（用于实验复现）。

### `fill_()`

`fill_()` 是 PyTorch 张量的一个**原地操作（in-place operation）** 方法，用于将张量的**所有元素**设置为指定的标量值。

```python
fill_(value) -> Tensor
```

- **`fill`**：表示"填充"的意思
- **下划线 `_`**：表示这是一个原地操作，会直接修改调用它的张量本身

## 3 `torch.nn`

### `torch.nn.MSELoss`

#### 1. 定义

`nn.MSELoss` 是 PyTorch 中用于计算**均方误差（Mean Squared Error, MSE）** 的损失函数类。它是回归问题中最常用、最基础的损失函数之一。

**MSE 的数学定义：**

```
MSE = (1/n) * Σ(y_pred - y_true)²
```

其中：

- `y_pred` 是模型的预测值
- `y_true` 是真实的目标值
- `n` 是样本数量
- `Σ` 表示求和

MSE 衡量的是**预测值与真实值之间差异的平方的平均值**。它惩罚较大的误差更为严重（因为平方操作），这使得模型更倾向于避免产生大的预测误差。

#### 2. 参数

```python
torch.nn.MSELoss(reduction='mean')
```

**主要参数：**

- **`reduction`** (字符串)：指定缩减方式，这是最重要的参数
  - **`'mean'`**：默认值。计算所有元素的平均值 `(1/n) * Σloss`
  - **`'sum'`**：计算所有元素的和 `Σloss`
  - **`'none'`**：不进行缩减，返回与输入相同形状的损失张量

### `torch.nn.Linear`

#### 1. 它是什么？

`nn.Linear` 是 PyTorch 的 `torch.nn` 模块中的一个类，它用于定义一个**全连接层**（Fully Connected Layer），也叫**线性层**或**仿射变换层**。

它的核心作用就是执行一个**线性变换**：

```
输出 = 输入 · 权重^T + 偏置
```

- `权重^T` 表示权重的转置。这是为了满足矩阵乘法的维度要求。
- 如果设置了 `bias=False`，则没有 `+ 偏置` 这一步。

#### 2. 语法和参数

python

```python
torch.nn.Linear(in_features, out_features, bias=True)
```

- **`in_features`**： 每个输入样本的特征数量（即输入张量最后一个维度的大小）。
- **`out_features`**： 每个输出样本的特征数量（即输出张量最后一个维度的大小）。
- **`bias`**： 一个布尔值。如果设置为 `False`，该层将不会学习一个附加的偏置项。默认为 `True`。

#### 3. 它的工作方式（输入和输出形状）

- **输入形状**: `(*, H_in)`
  - 这里的 `*` 表示可以有任意多个维度（例如，`(batch_size, H_in)` 或 `(batch_size, seq_len, H_in)` 等）。
  - `H_in` 必须等于 `in_features`。
- **输出形状**: `(*, H_out)`
  - 除了最后一个维度被变换为 `H_out`（即 `out_features`）外，所有其他维度都保持不变。

**最常见的情况**：输入是一个二维张量，形状为 `(N, C_in)`，其中 `N` 是批次大小（batch size），`C_in` 是特征数。
那么输出就是一个二维张量，形状为 `(N, C_out)`。

#### 4. 内部参数

一个 `nn.Linear` 模块包含两个可学习的参数：

- **`weight`**: 权重矩阵，形状为 `(out_features, in_features)`。
- **`bias`**: 偏置向量，形状为 `(out_features,)`（如果 `bias=True`）。

这些参数在模块初始化时被随机初始化（例如使用 Kaiming 或 Xavier 初始化方法），并在模型训练过程中通过反向传播不断更新。

你可以通过 `linear_layer.weight` 和 `linear_layer.bias` 来访问它们。

#### 5. 简单代码示例

让我们通过几个例子来直观地理解它。

**示例 1：基础用法**

```python
import torch
import torch.nn as nn

# 定义一个线性层：输入特征数为 5，输出特征数为 3
linear_layer = nn.Linear(in_features=5, out_features=3, bias=True)

# 创建一个随机输入张量，形状为 (batch_size, in_features) = (2, 5)
input_tensor = torch.randn(2, 5)
print("Input shape:", input_tensor.shape) # torch.Size([2, 5])

# 将输入传递给线性层
output_tensor = linear_layer(input_tensor)
print("Output shape:", output_tensor.shape) # torch.Size([2, 3])

# 查看内部参数
print("Weight shape:", linear_layer.weight.shape) # torch.Size([3, 5])
print("Bias shape:", linear_layer.bias.shape)   # torch.Size([3])
```



**示例 2：验证计算过程**

我们可以手动进行矩阵乘法来验证 `nn.Linear` 的结果。

```python
import torch
import torch.nn as nn

# 为了复现结果，设置随机种子
torch.manual_seed(42)

# 定义一个非常小的层
linear_layer = nn.Linear(2, 1, bias=True)
print("Initial weight:", linear_layer.weight) # e.g., tensor([[0.6641, 0.1634]])
print("Initial bias:", linear_layer.bias)   # e.g., tensor([-0.3561])

# 创建一个简单的输入
input_tensor = torch.tensor([[1.0, 2.0]])
output = linear_layer(input_tensor)
print("Output from nn.Linear:", output) # e.g., tensor([[0.8980]])

# 手动计算验证
manual_output = torch.matmul(input_tensor, linear_layer.weight.T) + linear_layer.bias
print("Manual calculation:", manual_output) # e.g., tensor([[0.8980]])

# 检查两者是否相等（允许微小的浮点数误差）
print("Are they close?", torch.allclose(output, manual_output)) # True
```



**示例 3：更高维度的输入**

`nn.Linear` 只对最后一个维度进行操作，前面的维度都会被保留。

```python
import torch
import torch.nn as nn

linear_layer = nn.Linear(10, 5)

# 输入一个三维张量，形状为 (batch_size, sequence_length, features)
# 常见于处理序列数据的RNN/LSTM/Transformer模型
input_3d = torch.randn(4, 7, 10) # (batch_size=4, seq_len=7, in_features=10)
output_3d = linear_layer(input_3d)
print("3D Input shape:", input_3d.shape)  # torch.Size([4, 7, 10])
print("3D Output shape:", output_3d.shape) # torch.Size([4, 7, 5]) features从10变为了5

# 输入一个四维张量（例如，卷积特征图展平后的结果）
input_4d = torch.randn(4, 16, 8, 10) # 假设 16*8=128个空间位置，每个位置有10个特征
output_4d = linear_layer(input_4d)
print("4D Input shape:", input_4d.shape)  # torch.Size([4, 16, 8, 10])
print("4D Output shape:", output_4d.shape) # torch.Size([4, 16, 8, 5])
```

#### 6. 在神经网络中的使用

`nn.Linear` 是构建深度学习模型最基础的积木块，几乎无处不在。

```python
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        # 第一个全连接层：input_size -> hidden_size
        self.fc1 = nn.Linear(input_size, hidden_size)
        # 激活函数，引入非线性
        self.relu = nn.ReLU()
        # 第二个全连接层（输出层）：hidden_size -> num_classes
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# 实例化模型
model = SimpleNet(input_size=784, hidden_size=500, num_classes=10)
# 输入一个MNIST图片（展平为784维向量）
x = torch.randn(64, 784) # batch_size=64
output = model(x)
print(output.shape) # torch.Size([64, 10])
```

### `torch.nn.Sequential`

#### 1. 它是什么？

`nn.Sequential` 是一个**容器（Container）模块**，它允许你将多个网络模块**按顺序**组合在一起，形成一个更大的模块。数据会按照你在 Sequential 中定义的顺序，依次通过每一个子模块。

可以把它想象成一个**流水线**或者**一叠积木**：输入数据从一端进入，依次经过每个处理步骤，最后从另一端输出。

#### 2. 为什么需要它？

在没有 `nn.Sequential` 的情况下，构建一个简单的多层网络需要在 `forward` 函数中手动调用每一层：

```python
class MyNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x))) # 手动调用每一层
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

使用 `nn.Sequential` 可以：
1.  **简化代码**：避免在 `forward` 中写重复的调用语句。
2.  **提高可读性**：网络结构一目了然。
3.  **方便管理**：整个 Sequential 块被视为一个整体，可以轻松打印、遍历或提取特征。

#### 3. 语法和创建方式

有几种创建 `nn.Sequential` 的方法：

**方式一：按顺序直接传递模块（最常用）**
```python
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)
```

**方式二：使用 OrderedDict（推荐，可以命名每个层）**

```python
from collections import OrderedDict

model = nn.Sequential(OrderedDict([
    ('flatten', nn.Flatten()),
    ('fc1', nn.Linear(784, 256)),
    ('relu1', nn.ReLU()),
    ('dropout1', nn.Dropout(0.2)),
    ('fc2', nn.Linear(256, 128)),
    ('relu2', nn.ReLU()),
    ('fc3', nn.Linear(128, 10))
]))
```
这种方式的好处是每一层都有了一个名字，在打印模型或调试时非常清晰。

**方式三：使用 add_module 方法动态添加**
```python
model = nn.Sequential()
model.add_module('conv1', nn.Conv2d(3, 6, 5))
model.add_module('pool1', nn.MaxPool2d(2))
model.add_module('conv2', nn.Conv2d(6, 16, 5))
```

#### 4. 它的工作方式（前向传播）

`nn.Sequential` 的前向传播 `forward(x)` 非常简单：**按顺序**将输入 `x` 传递给它的第一个子模块，然后将结果传递给第二个子模块，依此类推，直到最后一个子模块产生输出。

```python
# 对于上面的第一个例子，这行代码：
output = model(input_data)

# 等价于手动执行了：
x = nn.Linear(784, 256)(input_data)
x = nn.ReLU()(x)
x = nn.Linear(256, 128)(x)
x = nn.ReLU()(x)
x = nn.Linear(128, 10)(x)
output = x
```

#### 5. 访问内部模块和参数

你可以像访问列表一样通过索引来访问 `nn.Sequential` 中的子模块，也可以通过你赋予的名字（如果使用了 OrderedDict 或 `add_module`）来访问。

```python
# 使用方式一的模型
print(model[0]) # 打印第一层: Linear(in_features=784, out_features=256, bias=True)
print(model[2]) # 打印第三层: Linear(in_features=256, out_features=128, bias=True)

# 使用方式二的模型（命名后）
print(model.fc1)    # 访问名为 'fc1' 的层
print(model.relu2)  # 访问名为 'relu2' 的层

# 遍历所有子模块
for name, module in model.named_children():
    print(f"Layer: {name} | Module: {module}")

# 获取第0层的参数
first_layer_weights = model[0].weight
```

#### 6. 完整代码示例

让我们看一个结合了卷积层和全连接层的完整 CNN 例子：

```python
import torch
import torch.nn as nn
from collections import OrderedDict

# 定义一个CNN模型，使用Sequential组织不同的部分
class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        
        # 特征提取部分 (卷积 -> 激活 -> 池化)
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        # 分类器部分 (全连接层)
        self.classifier = nn.Sequential(OrderedDict([
            ('dropout1', nn.Dropout(p=0.5)),
            ('fc1', nn.Linear(64 * 8 * 8, 512)), # 假设输入图像是32x32，经过两次池化后为8x8
            ('relu1', nn.ReLU(inplace=True)),
            ('dropout2', nn.Dropout(p=0.5)),
            ('fc2', nn.Linear(512, num_classes)),
        ]))

    def forward(self, x):
        x = self.features(x) # 数据流经features Sequential
        x = torch.flatten(x, 1) # 展平特征图
        x = self.classifier(x) # 数据流经classifier Sequential
        return x

# 创建模型实例
model = CNN(num_classes=10)
print(model)

# 创建一个随机输入（batch_size=4, 3通道, 32x32图像）
dummy_input = torch.randn(4, 3, 32, 32)
output = model(dummy_input)
print(f"Input shape: {dummy_input.shape}")
print(f"Output shape: {output.shape}") # 应该是 torch.Size([4, 10])
```

**输出示例：**
```
CNN(
  (features): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (dropout1): Dropout(p=0.5, inplace=False)
    (fc1): Linear(in_features=4096, out_features=512, bias=True)
    (relu1): ReLU(inplace=True)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
)
Input shape: torch.Size([4, 3, 32, 32])
Output shape: torch.Size([4, 10])
```

## 4 `torch.optim`

### `torch.optim.SGD`

#### 1. 它是什么？

`torch.optim.SGD` 是 PyTorch 中实现**随机梯度下降（Stochastic Gradient Descent）** 算法的优化器类。它是深度学习中最基础、最核心的优化算法，许多更先进的优化器（如 Adam）都是基于 SGD 的改进。

#### 2. 核心思想

SGD 的核心思想很简单：**沿着损失函数的负梯度方向更新参数**，从而最小化损失函数。

基本更新公式：
```
param = param - learning_rate * param.grad
```

#### 3. 语法和参数

```python
torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, 
               weight_decay=0, nesterov=False, *, maximize=False)
```

**主要参数详解：**

| 参数               | 说明                                          | 默认值 |
| ------------------ | --------------------------------------------- | ------ |
| **`params`**       | 需要优化的参数（通常是 `model.parameters()`） | 必需   |
| **`lr`**           | **学习率** - 控制每次参数更新的步长           | 必需   |
| **`momentum`**     | **动量** - 加速收敛，减少振荡                 | 0      |
| **`weight_decay`** | **权重衰减** - L2 正则化系数                  | 0      |
| **`nesterov`**     | 是否使用 Nesterov 动量                        | False  |
| **`dampening`**    | 动量阻尼系数                                  | 0      |
| **`maximize`**     | 是否最大化目标函数（而不是最小化）            | False  |

#### 4. 基本用法示例

##### 示例 1：最简单的 SGD
```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的模型
model = nn.Linear(10, 1)
criterion = nn.MSELoss()

# 创建SGD优化器（最基本的形式）
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 模拟训练数据
x = torch.randn(5, 10)  # batch_size=5, features=10
y = torch.randn(5, 1)   # 目标值

# 训练步骤
for epoch in range(100):
    # 前向传播
    predictions = model(x)
    loss = criterion(predictions, y)
    
    # 反向传播
    optimizer.zero_grad()  # 清除之前的梯度
    loss.backward()        # 计算梯度
    
    # 参数更新
    optimizer.step()       # 执行SGD更新
    
    if epoch % 20 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
```

#### 5. 动量（Momentum）的作用

动量帮助优化器在相关方向上加速，减少振荡。

##### 示例 2：使用动量的 SGD
```python
# 使用动量（通常设为0.9）
optimizer_with_momentum = optim.SGD(
    model.parameters(), 
    lr=0.01, 
    momentum=0.9
)

# 更新公式变为：
# v = momentum * v - lr * grad
# param = param + v
```

#### 6. 权重衰减（L2正则化）

权重衰减通过在损失函数中添加L2正则项来防止过拟合。

##### 示例 3：使用权重衰减
```python
# 添加权重衰减（L2正则化）
optimizer_with_wd = optim.SGD(
    model.parameters(), 
    lr=0.01, 
    weight_decay=1e-4  # 常用的权重衰减系数
)

# 这等价于在损失函数中添加：weight_decay * Σ(param^2)
```

#### 7. Nesterov 动量

Nesterov 动量是标准动量的改进版本，通常能提供更好的收敛性能。

##### 示例 4：使用 Nesterov 动量
```python
optimizer_nesterov = optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    nesterov=True  # 启用Nesterov动量
)
```

#### 8. 完整训练示例

##### 示例 5：完整的神经网络训练
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. 准备数据
x_train = torch.randn(1000, 10)
y_train = torch.randn(1000, 1)
dataset = TensorDataset(x_train, y_train)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 2. 定义模型
model = nn.Sequential(
    nn.Linear(10, 50),
    nn.ReLU(),
    nn.Linear(50, 1)
)

# 3. 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(
    model.parameters(),
    lr=0.01,           # 学习率
    momentum=0.9,       # 动量
    weight_decay=1e-4,  # 权重衰减
    nesterov=True       # Nesterov动量
)

# 4. 训练循环
num_epochs = 50
for epoch in range(num_epochs):
    epoch_loss = 0.0
    
    for batch_x, batch_y in dataloader:
        # 前向传播
        predictions = model(batch_x)
        loss = criterion(predictions, batch_y)
        
        # 反向传播
        optimizer.zero_grad()  # 重要：清除之前的梯度
        loss.backward()
        
        # 参数更新
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(dataloader)
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')
```

#### 9. 学习率调度

通常需要随着训练进行调整学习率。

##### 示例 6：学习率调度
```python
from torch.optim.lr_scheduler import StepLR

optimizer = optim.SGD(model.parameters(), lr=0.1)

# 每30个epoch将学习率乘以0.1
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(100):
    # 训练步骤...
    train(...)
    
    # 更新学习率
    scheduler.step()
    
    current_lr = optimizer.param_groups[0]['lr']
    print(f'Epoch {epoch}, Learning Rate: {current_lr}')
```

#### 10. 参数组（Parameter Groups）

可以对不同的参数设置不同的超参数。

##### 示例 7：不同的参数不同的学习率
```python
# 获取模型的参数
params = list(model.named_parameters())

# 为不同层设置不同的学习率
optimizer = optim.SGD([
    {'params': model[0].parameters(), 'lr': 0.01},  # 第一层
    {'params': model[2].parameters(), 'lr': 0.001}  # 第三层
], momentum=0.9)
```

#### 11. SGD 的变体和技巧

##### 示例 8：带动量的SGD实际效果
```python
# 对比有动量和没有动量的收敛速度
model1 = nn.Linear(10, 1)
model2 = nn.Linear(10, 1)

optimizer_no_momentum = optim.SGD(model1.parameters(), lr=0.01)
optimizer_with_momentum = optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)

# 通常 optimizer_with_momentum 会收敛得更快
```

